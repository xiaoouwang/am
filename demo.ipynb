{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8899bcc2-919b-41b3-914f-b3a549f9b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch nlpbaselines evaluate seqeval accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2447f749-8692-4a9d-a264-c629b1c9a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ef3fe2-ee67-4cd8-a19e-dcc44c96a76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 372,\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['Should',\n",
       "  'governments',\n",
       "  'spend',\n",
       "  'more',\n",
       "  'money',\n",
       "  'on',\n",
       "  'improving',\n",
       "  'roads',\n",
       "  'and',\n",
       "  'highways',\n",
       "  'Nowadays',\n",
       "  'transportation',\n",
       "  'has',\n",
       "  'attracted',\n",
       "  'much',\n",
       "  'social',\n",
       "  'concern',\n",
       "  '.',\n",
       "  'Whether',\n",
       "  'governments',\n",
       "  'should',\n",
       "  'invest',\n",
       "  'more',\n",
       "  'money',\n",
       "  'in',\n",
       "  'traffic',\n",
       "  'infrastructure',\n",
       "  'or',\n",
       "  'focus',\n",
       "  'more',\n",
       "  'on',\n",
       "  'developing',\n",
       "  'public',\n",
       "  'transportation',\n",
       "  'has',\n",
       "  'erupted',\n",
       "  'endless',\n",
       "  'controversy',\n",
       "  '.',\n",
       "  'It',\n",
       "  'does',\n",
       "  'not',\n",
       "  'seem',\n",
       "  'unreasonable',\n",
       "  'to',\n",
       "  'suggest',\n",
       "  'that',\n",
       "  'governments',\n",
       "  'spend',\n",
       "  'more',\n",
       "  'money',\n",
       "  'on',\n",
       "  'buses',\n",
       "  ',',\n",
       "  'trains',\n",
       "  ',',\n",
       "  'and',\n",
       "  'subways',\n",
       "  'investment',\n",
       "  '.',\n",
       "  'Such',\n",
       "  'public',\n",
       "  'vehicles',\n",
       "  'are',\n",
       "  'used',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'a',\n",
       "  'large',\n",
       "  'number',\n",
       "  'of',\n",
       "  'people',\n",
       "  'in',\n",
       "  'society',\n",
       "  'therefore',\n",
       "  'they',\n",
       "  'help',\n",
       "  'alleviate',\n",
       "  'traffic',\n",
       "  'congestion',\n",
       "  'and',\n",
       "  'decrease',\n",
       "  'the',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'carbon',\n",
       "  'released',\n",
       "  'into',\n",
       "  'the',\n",
       "  'air',\n",
       "  '.',\n",
       "  'Both',\n",
       "  'these',\n",
       "  'two',\n",
       "  'things',\n",
       "  'are',\n",
       "  'important',\n",
       "  '.',\n",
       "  'First',\n",
       "  'and',\n",
       "  'foremost',\n",
       "  ',',\n",
       "  'carbon',\n",
       "  'emission',\n",
       "  'cut',\n",
       "  'is',\n",
       "  'significantly',\n",
       "  'essential',\n",
       "  'for',\n",
       "  'protecting',\n",
       "  'the',\n",
       "  'atmosphere',\n",
       "  '.',\n",
       "  'The',\n",
       "  'fact',\n",
       "  'is',\n",
       "  'that',\n",
       "  'the',\n",
       "  'more',\n",
       "  'cars',\n",
       "  'and',\n",
       "  'motorbikes',\n",
       "  'are',\n",
       "  'on',\n",
       "  'roads',\n",
       "  ',',\n",
       "  'the',\n",
       "  'more',\n",
       "  'seriously',\n",
       "  'the',\n",
       "  'ozone',\n",
       "  'layer',\n",
       "  'is',\n",
       "  'damaged',\n",
       "  '.',\n",
       "  'If',\n",
       "  'governments',\n",
       "  'use',\n",
       "  'more',\n",
       "  'money',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'roads',\n",
       "  ',',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'strong',\n",
       "  'likelihood',\n",
       "  'that',\n",
       "  'more',\n",
       "  'people',\n",
       "  'drive',\n",
       "  'their',\n",
       "  'private',\n",
       "  'cars',\n",
       "  'work',\n",
       "  '.',\n",
       "  'This',\n",
       "  'is',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'lead',\n",
       "  'to',\n",
       "  'more',\n",
       "  'carbon',\n",
       "  'emitted',\n",
       "  'into',\n",
       "  'the',\n",
       "  'atmosphere',\n",
       "  ',',\n",
       "  'which',\n",
       "  'can',\n",
       "  'cause',\n",
       "  'skin',\n",
       "  'cancer',\n",
       "  'and',\n",
       "  'destroy',\n",
       "  'the',\n",
       "  'natural',\n",
       "  'environment',\n",
       "  '.',\n",
       "  'Whereas',\n",
       "  ',',\n",
       "  'if',\n",
       "  'there',\n",
       "  'are',\n",
       "  'more',\n",
       "  'good',\n",
       "  'buses',\n",
       "  ',',\n",
       "  'trains',\n",
       "  ',',\n",
       "  'or',\n",
       "  'subways',\n",
       "  ',',\n",
       "  'people',\n",
       "  'are',\n",
       "  'inclined',\n",
       "  'to',\n",
       "  'use',\n",
       "  'less',\n",
       "  'private',\n",
       "  'vehicles',\n",
       "  ',',\n",
       "  'which',\n",
       "  'decreases',\n",
       "  'the',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'carbon',\n",
       "  'released',\n",
       "  '.',\n",
       "  'Obviously',\n",
       "  ',',\n",
       "  'the',\n",
       "  'policy',\n",
       "  'that',\n",
       "  'concentrates',\n",
       "  'money',\n",
       "  'on',\n",
       "  'developing',\n",
       "  'public',\n",
       "  'transportation',\n",
       "  'brings',\n",
       "  'an',\n",
       "  'advantageous',\n",
       "  'impact',\n",
       "  'on',\n",
       "  'earth',\n",
       "  '.',\n",
       "  'In',\n",
       "  'addition',\n",
       "  ',',\n",
       "  'the',\n",
       "  'policy',\n",
       "  'would',\n",
       "  'play',\n",
       "  'a',\n",
       "  'pivotal',\n",
       "  'part',\n",
       "  'in',\n",
       "  'relieving',\n",
       "  'traffic',\n",
       "  'jam',\n",
       "  '.',\n",
       "  'It',\n",
       "  'is',\n",
       "  'quiet',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'traffic',\n",
       "  'congestion',\n",
       "  'is',\n",
       "  'a',\n",
       "  'serious',\n",
       "  'problem',\n",
       "  'in',\n",
       "  'many',\n",
       "  'big',\n",
       "  'cities',\n",
       "  '.',\n",
       "  'The',\n",
       "  'main',\n",
       "  'reason',\n",
       "  'for',\n",
       "  'this',\n",
       "  'issue',\n",
       "  'is',\n",
       "  'that',\n",
       "  'there',\n",
       "  'are',\n",
       "  'a',\n",
       "  'great',\n",
       "  'number',\n",
       "  'of',\n",
       "  'vehicles',\n",
       "  'flowing',\n",
       "  'on',\n",
       "  'roads',\n",
       "  '.',\n",
       "  'In',\n",
       "  'order',\n",
       "  'to',\n",
       "  'tackle',\n",
       "  'the',\n",
       "  'problem',\n",
       "  ',',\n",
       "  'governments',\n",
       "  'have',\n",
       "  'to',\n",
       "  'turn',\n",
       "  'people',\n",
       "  \"'\",\n",
       "  'attention',\n",
       "  'on',\n",
       "  'public',\n",
       "  'vehicles',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'cars',\n",
       "  'and',\n",
       "  'motorbikes',\n",
       "  '.',\n",
       "  'There',\n",
       "  'is',\n",
       "  'the',\n",
       "  'possibility',\n",
       "  'that',\n",
       "  'improving',\n",
       "  'roads',\n",
       "  'and',\n",
       "  'highways',\n",
       "  'does',\n",
       "  'not',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'reach',\n",
       "  'the',\n",
       "  'major',\n",
       "  'cause',\n",
       "  'of',\n",
       "  'traffic',\n",
       "  'congestion',\n",
       "  '.',\n",
       "  'In',\n",
       "  'conclusion',\n",
       "  ',',\n",
       "  'I',\n",
       "  'would',\n",
       "  'concede',\n",
       "  'that',\n",
       "  'high',\n",
       "  '-',\n",
       "  'quality',\n",
       "  'roads',\n",
       "  'and',\n",
       "  'highways',\n",
       "  'make',\n",
       "  'it',\n",
       "  'easier',\n",
       "  'to',\n",
       "  'move',\n",
       "  'around',\n",
       "  '.',\n",
       "  'Nevertheless',\n",
       "  ',',\n",
       "  'public',\n",
       "  'transportation',\n",
       "  'systems',\n",
       "  'even',\n",
       "  'have',\n",
       "  'more',\n",
       "  'important',\n",
       "  'advantages',\n",
       "  '.',\n",
       "  'They',\n",
       "  'not',\n",
       "  'only',\n",
       "  'ease',\n",
       "  'traffic',\n",
       "  'jam',\n",
       "  'but',\n",
       "  'also',\n",
       "  'protect',\n",
       "  'the',\n",
       "  'atmosphere',\n",
       "  '.',\n",
       "  'That',\n",
       "  'is',\n",
       "  'why',\n",
       "  'many',\n",
       "  'governments',\n",
       "  'around',\n",
       "  'the',\n",
       "  'world',\n",
       "  'invest',\n",
       "  'more',\n",
       "  'money',\n",
       "  'on',\n",
       "  'developing',\n",
       "  'public',\n",
       "  'transportation',\n",
       "  'facilities',\n",
       "  '.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset, I've already preprocessed it\n",
    "\n",
    "import pickle\n",
    "with open('ds_split.pickle', 'rb') as f:\n",
    "    ds_split = pickle.load(f)\n",
    "\n",
    "ds_split[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7641971-678b-4cd5-b9b9-856c4bc6cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Argument Mining as a Classical Sequence Tagging problem\n",
    "# Gather label information\n",
    "\n",
    "label_list = ds_split[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Claim\": 1,\n",
    "    \"I-Claim\": 2,\n",
    "    \"B-Majorclaim\": 3,\n",
    "    \"I-Majorclaim\": 4,\n",
    "    \"B-Premise\": 5,\n",
    "    \"I-Premise\": 6\n",
    "}\n",
    "\n",
    "id2label = {v:k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127309b0-0c4a-4f9d-b7e8-9ff608633d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████| 361/361 [00:00<00:00, 1214.92 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████| 41/41 [00:00<00:00, 1241.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True) # add prefix space necessary for Roberta\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    # truncation is true because the input > 512 in like 24 documents\n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(batch[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens (CLS and SEP) to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "    \n",
    "tokenized_ds = ds_split.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa880d89-6035-4b75-b307-b946984b4d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Setting\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "seqeval = evaluate.load(\"seqeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fdc7295-8695-4a9d-abee-9d64ed870f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='724' max='724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [724/724 00:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.518309</td>\n",
       "      <td>0.297441</td>\n",
       "      <td>0.464226</td>\n",
       "      <td>0.362573</td>\n",
       "      <td>0.800714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.415121</td>\n",
       "      <td>0.485030</td>\n",
       "      <td>0.673877</td>\n",
       "      <td>0.564067</td>\n",
       "      <td>0.852892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.409263</td>\n",
       "      <td>0.534106</td>\n",
       "      <td>0.690516</td>\n",
       "      <td>0.602322</td>\n",
       "      <td>0.853565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.434051</td>\n",
       "      <td>0.537371</td>\n",
       "      <td>0.693844</td>\n",
       "      <td>0.605664</td>\n",
       "      <td>0.856931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the smaller the batch size the better in this case\n",
    "\n",
    "# Tested\n",
    "# batch_sizes = [2, 4, 8, 16]\n",
    "# learning_rates = [2e-5, 3e-5, 4e-5, 5e-5]\n",
    "\n",
    "# 2 3e-05 the best, 3rd epoch before overfitting\n",
    "\n",
    "batch_sizes = [2]\n",
    "learning_rates = [3e-5]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        print(batch_size, learning_rate)\n",
    "        # reinitiate the model for each combination\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id).to(device)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"results-roberta/{batch_size}-{learning_rate}\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=4,   # overfitting often starts from 3\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            push_to_hub=False)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_ds[\"train\"],\n",
    "            eval_dataset=tokenized_ds[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics)\n",
    "        \n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f55427a-a595-41ae-a579-8bce1311930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Move 2-3e-05 to xiaoou for inference\n",
    "!mkdir models\n",
    "!mv results-roberta/2-3e-05/checkpoint-543 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2470ce6-332b-4913-92f2-b72bcf98593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'O',\n",
       "  'score': 0.9093813,\n",
       "  'word': ' It is always said that',\n",
       "  'start': 0,\n",
       "  'end': 22},\n",
       " {'entity_group': 'Claim',\n",
       "  'score': 0.6267374,\n",
       "  'word': ' competition can effectively promote the development of economy',\n",
       "  'start': 23,\n",
       "  'end': 85},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.40173453,\n",
       "  'word': '. In',\n",
       "  'start': 85,\n",
       "  'end': 89},\n",
       " {'entity_group': 'Premise',\n",
       "  'score': 0.86719596,\n",
       "  'word': ' order to survive in the competition, companies continue to improve their products and service',\n",
       "  'start': 90,\n",
       "  'end': 183},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.47686225,\n",
       "  'word': ',',\n",
       "  'start': 183,\n",
       "  'end': 184},\n",
       " {'entity_group': 'Premise',\n",
       "  'score': 0.6580801,\n",
       "  'word': ' and',\n",
       "  'start': 185,\n",
       "  'end': 188},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.7127421,\n",
       "  'word': ' as a result,',\n",
       "  'start': 189,\n",
       "  'end': 201},\n",
       " {'entity_group': 'Premise',\n",
       "  'score': 0.58878356,\n",
       "  'word': ' the whole society prospers',\n",
       "  'start': 202,\n",
       "  'end': 228},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.78179526,\n",
       "  'word': \". However, when we discuss the issue of competition or cooperation, what we are concerned about is not the whole society, but the development of an individual's whole life. From this point of view, I firmly believe that\",\n",
       "  'start': 228,\n",
       "  'end': 447},\n",
       " {'entity_group': 'Majorclaim',\n",
       "  'score': 0.91595584,\n",
       "  'word': ' we should attach more importance to cooperation during primary education',\n",
       "  'start': 448,\n",
       "  'end': 520},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.3056547,\n",
       "  'word': '.',\n",
       "  'start': 520,\n",
       "  'end': 521}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "text = \"It is always said that competition can effectively promote the development of economy. In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers. However, when we discuss the issue of competition or cooperation, what we are concerned about is not the whole society, but the development of an individual's whole life. From this point of view, I firmly believe that we should attach more importance to cooperation during primary education.\"\n",
    "\n",
    "model_path = \"models/checkpoint-543\"\n",
    "\n",
    "# ignore labels -> output all labels, grouped_entities = output trunks\n",
    "classifier = pipeline(\"ner\", model=model_path, grouped_entities=True, ignore_labels = [])\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d5f4d-be19-4f81-afdf-589e96cfbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to hugging face\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "# model.push_to_hub(\"xiaoou/am\",token=\"xx\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# tokenizer.push_to_hub(\"xiaoou/am\",token=\"xx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
